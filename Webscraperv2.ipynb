{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13193d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd    \n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ede68ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver's path\n",
    "path = '/Users/Kongs/Documents/ChromeDriver/chromedriver.exe'\n",
    "driver = webdriver.Chrome(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4c173dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximize Window\n",
    "driver.maximize_window() \n",
    "driver.minimize_window() \n",
    "driver.maximize_window() \n",
    "driver.switch_to.window(driver.current_window_handle)\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "590a3843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter to the site\n",
    "driver.get('https://www.linkedin.com/login');\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15b3147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept cookies\n",
    "driver.find_element_by_xpath('//*[@id=\"artdeco-global-alert-container\"]/div/section/div/div[2]/button[1]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f97c21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Credentials\n",
    "# Reading txt file where we have our user credentials\n",
    "with open('user_credentials.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    user_credentials = file.readlines()\n",
    "    user_credentials = [line.rstrip() for line in user_credentials]\n",
    "user_name = user_credentials[0] # First line\n",
    "password = user_credentials[1] # Second line\n",
    "driver.find_element_by_xpath('//*[@id=\"username\"]').send_keys(user_name)\n",
    "driver.find_element_by_xpath('//*[@id=\"password\"]').send_keys(password)\n",
    "time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10c898fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login button\n",
    "driver.find_element_by_xpath('//*[@id=\"organic-div\"]/form/div[3]/button').click()\n",
    "driver.implicitly_wait(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2ea5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jobs page\n",
    "driver.find_element_by_xpath('//*[@id=\"global-nav\"]/div/nav/ul/li[3]/a').click()\n",
    "time.sleep(3)\n",
    "# Go to search results directly via link\n",
    "#driver.get('https://www.linkedin.com/jobs/search/?currentJobId=3361761583&geoId=104514075&keywords=Remote%20Sensing&location=Denmark&refresh=true')\n",
    "driver.get('https://www.linkedin.com/jobs/search/?currentJobId=3371089139&geoId=103977389&keywords=Remote%20Sensing&location=Washington%2C%20United%20States&refresh=true')\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57fadb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all links for these offers\n",
    "links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2c05148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links are being collected now.\n",
      "Collecting the links in the page: 1\n",
      "Collecting the links in the page: 2\n",
      "Collecting the links in the page: 3\n",
      "Collecting the links in the page: 4\n",
      "Found 99 links for job offers\n"
     ]
    }
   ],
   "source": [
    "# Navigate 5 pages\n",
    "\n",
    "print('Links are being collected now.')\n",
    "try: \n",
    "    for page in range(2,6):\n",
    "        time.sleep(2)\n",
    "        jobs_block = driver.find_element_by_class_name('jobs-search-results-list')\n",
    "        jobs_list= jobs_block.find_elements(By.CSS_SELECTOR, '.jobs-search-results__list-item')\n",
    "    \n",
    "        for job in jobs_list:\n",
    "            all_links = job.find_elements_by_tag_name('a')\n",
    "            for a in all_links:\n",
    "                if str(a.get_attribute('href')).startswith(\"https://www.linkedin.com/jobs/view\") and a.get_attribute('href') not in links: \n",
    "                    links.append(a.get_attribute('href'))\n",
    "                else:\n",
    "                    pass\n",
    "            # scroll down for each job element\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", job)\n",
    "        \n",
    "        print(f'Collecting the links in the page: {page-1}')\n",
    "        # go to next page:\n",
    "        driver.find_element_by_xpath(f\"//button[@aria-label='Page {page}']\").click()\n",
    "        time.sleep(3)\n",
    "except:\n",
    "    pass\n",
    "print('Found ' + str(len(links)) + ' links for job offers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6de4eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store information\n",
    "job_titles = []\n",
    "company_names = []\n",
    "company_locations = []\n",
    "work_methods = []\n",
    "post_dates = []\n",
    "work_times = [] \n",
    "job_desc = []\n",
    "\n",
    "i = 0\n",
    "j = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a33a339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting the links and collecting information just started.\n",
      "Scraping the Job Offer 1 DONE.\n",
      "Scraping the Job Offer 2\n",
      "Scraping the Job Offer 2 DONE.\n",
      "Scraping the Job Offer 3\n",
      "Scraping the Job Offer 3 DONE.\n",
      "Scraping the Job Offer 4\n",
      "Scraping the Job Offer 4 DONE.\n",
      "Scraping the Job Offer 5\n",
      "Scraping the Job Offer 5 DONE.\n",
      "Scraping the Job Offer 6\n",
      "Scraping the Job Offer 6 DONE.\n",
      "Scraping the Job Offer 7\n",
      "Scraping the Job Offer 7 DONE.\n",
      "Scraping the Job Offer 8\n",
      "Scraping the Job Offer 8 DONE.\n",
      "Scraping the Job Offer 9\n",
      "Scraping the Job Offer 9 DONE.\n",
      "Scraping the Job Offer 10\n",
      "Scraping the Job Offer 10 DONE.\n",
      "Scraping the Job Offer 11\n",
      "Scraping the Job Offer 11 DONE.\n",
      "Scraping the Job Offer 12\n",
      "Scraping the Job Offer 12 DONE.\n",
      "Scraping the Job Offer 13\n",
      "Scraping the Job Offer 13 DONE.\n",
      "Scraping the Job Offer 14\n",
      "Scraping the Job Offer 14 DONE.\n",
      "Scraping the Job Offer 15\n",
      "Scraping the Job Offer 15 DONE.\n",
      "Scraping the Job Offer 16\n",
      "Scraping the Job Offer 16 DONE.\n",
      "Scraping the Job Offer 17\n",
      "Scraping the Job Offer 17 DONE.\n",
      "Scraping the Job Offer 18\n",
      "Scraping the Job Offer 18 DONE.\n",
      "Scraping the Job Offer 19\n",
      "Scraping the Job Offer 19 DONE.\n",
      "Scraping the Job Offer 20\n",
      "Scraping the Job Offer 20 DONE.\n",
      "Scraping the Job Offer 21\n",
      "Scraping the Job Offer 21 DONE.\n",
      "Scraping the Job Offer 22\n",
      "Scraping the Job Offer 22 DONE.\n",
      "Scraping the Job Offer 23\n",
      "Scraping the Job Offer 23 DONE.\n",
      "Scraping the Job Offer 24\n",
      "Scraping the Job Offer 24 DONE.\n",
      "Scraping the Job Offer 25\n",
      "Scraping the Job Offer 25 DONE.\n",
      "Scraping the Job Offer 26\n",
      "Scraping the Job Offer 26 DONE.\n",
      "Scraping the Job Offer 27\n",
      "Scraping the Job Offer 27 DONE.\n",
      "Scraping the Job Offer 28\n",
      "Scraping the Job Offer 28 DONE.\n",
      "Scraping the Job Offer 29\n",
      "Scraping the Job Offer 29 DONE.\n",
      "Scraping the Job Offer 30\n",
      "Scraping the Job Offer 30 DONE.\n",
      "Scraping the Job Offer 31\n",
      "Scraping the Job Offer 31 DONE.\n",
      "Scraping the Job Offer 32\n",
      "Scraping the Job Offer 32 DONE.\n",
      "Scraping the Job Offer 33\n",
      "Scraping the Job Offer 33 DONE.\n",
      "Scraping the Job Offer 34\n",
      "Scraping the Job Offer 34 DONE.\n",
      "Scraping the Job Offer 35\n",
      "Scraping the Job Offer 35 DONE.\n",
      "Scraping the Job Offer 36\n",
      "Scraping the Job Offer 36 DONE.\n",
      "Scraping the Job Offer 37\n",
      "Scraping the Job Offer 37 DONE.\n",
      "Scraping the Job Offer 38\n",
      "Scraping the Job Offer 38 DONE.\n",
      "Scraping the Job Offer 39\n",
      "Scraping the Job Offer 39 DONE.\n",
      "Scraping the Job Offer 40\n",
      "Scraping the Job Offer 40 DONE.\n",
      "Scraping the Job Offer 41\n",
      "Scraping the Job Offer 41 DONE.\n",
      "Scraping the Job Offer 42\n",
      "Scraping the Job Offer 42 DONE.\n",
      "Scraping the Job Offer 43\n",
      "Scraping the Job Offer 43 DONE.\n",
      "Scraping the Job Offer 44\n",
      "Scraping the Job Offer 44 DONE.\n",
      "Scraping the Job Offer 45\n",
      "Scraping the Job Offer 45 DONE.\n",
      "Scraping the Job Offer 46\n",
      "Scraping the Job Offer 46 DONE.\n",
      "Scraping the Job Offer 47\n",
      "Scraping the Job Offer 47 DONE.\n",
      "Scraping the Job Offer 48\n",
      "Scraping the Job Offer 48 DONE.\n",
      "Scraping the Job Offer 49\n",
      "Scraping the Job Offer 49 DONE.\n",
      "Scraping the Job Offer 50\n",
      "Scraping the Job Offer 50 DONE.\n",
      "Scraping the Job Offer 51\n",
      "Scraping the Job Offer 51 DONE.\n",
      "Scraping the Job Offer 52\n",
      "Scraping the Job Offer 52 DONE.\n",
      "Scraping the Job Offer 53\n",
      "Scraping the Job Offer 53 DONE.\n",
      "Scraping the Job Offer 54\n",
      "Scraping the Job Offer 54 DONE.\n",
      "Scraping the Job Offer 55\n",
      "Scraping the Job Offer 55 DONE.\n",
      "Scraping the Job Offer 56\n",
      "Scraping the Job Offer 56 DONE.\n",
      "Scraping the Job Offer 57\n",
      "Scraping the Job Offer 57 DONE.\n",
      "Scraping the Job Offer 58\n",
      "Scraping the Job Offer 58 DONE.\n",
      "Scraping the Job Offer 59\n",
      "Scraping the Job Offer 59 DONE.\n",
      "Scraping the Job Offer 60\n",
      "Scraping the Job Offer 60 DONE.\n",
      "Scraping the Job Offer 61\n",
      "Scraping the Job Offer 61 DONE.\n",
      "Scraping the Job Offer 62\n",
      "Scraping the Job Offer 62 DONE.\n",
      "Scraping the Job Offer 63\n",
      "Scraping the Job Offer 63 DONE.\n",
      "Scraping the Job Offer 64\n",
      "Scraping the Job Offer 64 DONE.\n",
      "Scraping the Job Offer 65\n",
      "Scraping the Job Offer 65 DONE.\n",
      "Scraping the Job Offer 66\n",
      "Scraping the Job Offer 66 DONE.\n",
      "Scraping the Job Offer 67\n",
      "Scraping the Job Offer 67 DONE.\n",
      "Scraping the Job Offer 68\n",
      "Scraping the Job Offer 68 DONE.\n",
      "Scraping the Job Offer 69\n",
      "Scraping the Job Offer 69 DONE.\n",
      "Scraping the Job Offer 70\n",
      "Scraping the Job Offer 70 DONE.\n",
      "Scraping the Job Offer 71\n",
      "Scraping the Job Offer 71 DONE.\n",
      "Scraping the Job Offer 72\n",
      "Scraping the Job Offer 72 DONE.\n",
      "Scraping the Job Offer 73\n",
      "Scraping the Job Offer 73 DONE.\n",
      "Scraping the Job Offer 74\n",
      "Scraping the Job Offer 74 DONE.\n",
      "Scraping the Job Offer 75\n",
      "Scraping the Job Offer 75 DONE.\n",
      "Scraping the Job Offer 76\n",
      "Scraping the Job Offer 76 DONE.\n",
      "Scraping the Job Offer 77\n",
      "Scraping the Job Offer 77 DONE.\n",
      "Scraping the Job Offer 78\n",
      "Scraping the Job Offer 78 DONE.\n",
      "Scraping the Job Offer 79\n",
      "Scraping the Job Offer 79 DONE.\n",
      "Scraping the Job Offer 80\n",
      "Scraping the Job Offer 80 DONE.\n",
      "Scraping the Job Offer 81\n",
      "Scraping the Job Offer 81 DONE.\n",
      "Scraping the Job Offer 82\n",
      "Scraping the Job Offer 82 DONE.\n",
      "Scraping the Job Offer 83\n",
      "Scraping the Job Offer 83 DONE.\n",
      "Scraping the Job Offer 84\n",
      "Scraping the Job Offer 84 DONE.\n",
      "Scraping the Job Offer 85\n",
      "Scraping the Job Offer 85 DONE.\n",
      "Scraping the Job Offer 86\n",
      "Scraping the Job Offer 86 DONE.\n",
      "Scraping the Job Offer 87\n",
      "Scraping the Job Offer 87 DONE.\n",
      "Scraping the Job Offer 88\n",
      "Scraping the Job Offer 88 DONE.\n",
      "Scraping the Job Offer 89\n",
      "Scraping the Job Offer 89 DONE.\n",
      "Scraping the Job Offer 90\n",
      "Scraping the Job Offer 90 DONE.\n",
      "Scraping the Job Offer 91\n",
      "Scraping the Job Offer 91 DONE.\n",
      "Scraping the Job Offer 92\n",
      "Scraping the Job Offer 92 DONE.\n",
      "Scraping the Job Offer 93\n",
      "Scraping the Job Offer 93 DONE.\n",
      "Scraping the Job Offer 94\n",
      "Scraping the Job Offer 94 DONE.\n",
      "Scraping the Job Offer 95\n",
      "Scraping the Job Offer 95 DONE.\n",
      "Scraping the Job Offer 96\n",
      "Scraping the Job Offer 96 DONE.\n",
      "Scraping the Job Offer 97\n",
      "Scraping the Job Offer 97 DONE.\n",
      "Scraping the Job Offer 98\n",
      "Scraping the Job Offer 98 DONE.\n",
      "Scraping the Job Offer 99\n",
      "Scraping the Job Offer 99 DONE.\n",
      "Scraping the Job Offer 100\n"
     ]
    }
   ],
   "source": [
    "# Visit each link one by one to scrape the information\n",
    "print('Visiting the links and collecting information just started.')\n",
    "for i in range(len(links)):\n",
    "    try:\n",
    "        driver.get(links[i])\n",
    "        i=i+1\n",
    "        time.sleep(2)\n",
    "        # Click See more.\n",
    "        driver.find_element_by_class_name(\"artdeco-card__actions\").click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Find the general information of the job offers\n",
    "    contents = driver.find_elements_by_class_name('p5')\n",
    "    for content in contents:\n",
    "        try:\n",
    "            job_titles.append(content.find_element_by_tag_name(\"h1\").text)\n",
    "            company_names.append(content.find_element_by_class_name(\"jobs-unified-top-card__company-name\").text)\n",
    "            company_locations.append(content.find_element_by_class_name(\"jobs-unified-top-card__bullet\").text)\n",
    "            work_methods.append(content.find_element_by_class_name(\"jobs-unified-top-card__workplace-type\").text)\n",
    "            post_dates.append(content.find_element_by_class_name(\"jobs-unified-top-card__posted-date\").text)\n",
    "            work_times.append(content.find_element_by_class_name(\"jobs-unified-top-card__job-insight\").text)\n",
    "            print(f'Scraping the Job Offer {j} DONE.')\n",
    "            j+= 1\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Scraping the job description\n",
    "    job_description = driver.find_elements_by_class_name('jobs-description__content')\n",
    "    for description in job_description:\n",
    "        job_text = description.find_element_by_class_name(\"jobs-box__html-content\").text\n",
    "        job_desc.append(job_text)\n",
    "        print(f'Scraping the Job Offer {j}')\n",
    "        time.sleep(2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e758c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataframe \n",
    "df = pd.DataFrame(list(zip(job_titles,company_names,\n",
    "                    company_locations,work_methods,\n",
    "                    post_dates,work_times)),\n",
    "                    columns =['job_title', 'company_name',\n",
    "                           'company_location','work_method',\n",
    "                           'post_date','work_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d75f069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the data to csv file\n",
    "df.to_csv('job_offers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c210f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output job descriptions to txt file\n",
    "with open('job_descriptions.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    for line in job_desc:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ffbe6f",
   "metadata": {},
   "source": [
    "*This webscraping notebook is provided by Mert Kucukkuru at https://medium.com/@kurumert/web-scraping-linkedin-job-page-with-selenium-python-e0b6183a5954*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
